{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_LSTM_base.ipynb","provenance":[],"authorship_tag":"ABX9TyOalIYUL/y6fnd1gD0uI5zP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GAe8iKFJJpe1"},"outputs":[],"source":["import torch\n","from torch import nn, optim\n","import numpy as np\n","\n","# Generate data\n","\n","sinewave = np.sin(np.arange(0, 2000, 0.1))\n","slices = sinewave.reshape(-1, 200)\n","input_tensor = torch.tensor(slices[:, :-1], dtype=torch.float).unsqueeze(2)\n","target_tensor = torch.tensor(slices[:, 1:], dtype=torch.float)\n","print(input_tensor.shape, target_tensor.shape)\n","\n","\n","# Model - seq2seq model with loop over decoder\n","\n","class Seq2SeqA(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, batch_size, sequence_length):\n","        super(Seq2SeqA, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.batch_size = batch_size\n","        self.sequence_length = sequence_length\n","        self.encoder_lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n","        self.decoder_lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers)\n","        self.linear = nn.Linear(self.hidden_size, 1)\n","\n","    def forward(self, input):\n","        _, hidden = self.encoder_lstm(input)\n","        input_t = torch.zeros(batch_size, 1, dtype=torch.float).unsqueeze(0)\n","        output_tensor = torch.zeros(sequence_length, batch_size, 1)\n","        for t in range(self.sequence_length):\n","            output_t, hidden = self.decoder_lstm(input_t, hidden)\n","            output_t = self.linear(output_t[-1])\n","            input_t = output_t.unsqueeze(0)\n","            output_tensor[t] = output_t\n","\n","        return output_tensor\n","\n","seq2seqA = Seq2SeqA(input_size=1, hidden_size=51, num_layers=1, batch_size=100, sequence_length=199)\n","\n","\n","# Training - seq2seq model with loop over decoder\n","\n","num_epochs = 300\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(seq2seqA.parameters(), lr=0.001)\n","\n","for epoch in range(num_epochs):\n","    optimizer.zero_grad()\n","    output = seq2seqA(input_tensor)\n","    output = output.squeeze().transpose(1,0)\n","    loss = criterion(output, target_tensor)\n","    loss.backward()\n","    optimizer.step()\n","\n","    if epoch % 10 == 0:\n","        print('Epoch: {} -- Training loss (MSE) {}'.format(epoch, loss.item()))\n","\n","\n","# Model - seq2seq model without loop over decoder\n","\n","class Seq2SeqB(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(Seq2SeqB, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n","        self.linear = nn.Linear(self.hidden_size, 1)\n","\n","    def forward(self, input):\n","        output, hidden = self.lstm(input)\n","        output = self.linear(output)\n","        return output\n","\n","seq2seqB = Seq2SeqB(input_size=1, hidden_size=51, num_layers=2)\n","\n","\n","# Training- seq2seq model without loop over decoder\n","\n","num_epochs = 300\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(seq2seqB.parameters(), lr=0.001)\n","\n","for epoch in range(num_epochs):\n","    optimizer.zero_grad()\n","    output = seq2seqB(input_tensor)\n","    output = output.squeeze()\n","    loss = criterion(output, target_tensor)\n","    loss.backward()\n","    optimizer.step()\n","\n","    if epoch % 10 == 0:\n","        print('Epoch: {} -- Training loss (MSE) {}'.format(epoch, loss.item()))\n"]}]}