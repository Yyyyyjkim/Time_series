{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer_v1_temp.py","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM0j1Ua2idhZ6ETkEKvWLqX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zIpjz03LCPmM"},"outputs":[],"source":["import pickle\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","import random\n","\n","from torch.utils.data import DataLoader, Dataset\n","import torch\n","from torch import Tensor\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from typing import Optional, Tuple\n","\n","\"\"\"\n","dataloader\n","\"\"\"\n","\n","class windowDataset(Dataset):\n","    def __init__(self, y, input_window, output_window, num_feature, stride=1):\n","        #총 데이터의 개수\n","        L = y.shape[0]\n","        # seq_len\n","        num_samples = (L - input_window - output_window) // stride + 1\n","\n","        #input과 output : shape = (window 크기, sample 개수)\n","        X = np.zeros([input_window, num_samples, num_feature])\n","        Y_src = np.zeros([output_window, num_samples, num_feature])\n","        Y_tgt = np.zeros([output_window, num_samples, num_feature])\n","\n","        for i in np.arange(num_samples):\n","            start_x = stride*i\n","            end_x = start_x + input_window\n","            X[:,i,:] = y[start_x:end_x]\n","\n","            start_y = stride*i + input_window \n","            end_y = start_y + output_window \n","            Y_src[:,i] = y[(start_y-1):(end_y-1)]\n","            Y_tgt[:,i] = y[start_y:end_y]\n","\n","        X = X.reshape(X.shape[0], X.shape[1], num_feature).transpose((1,0,2)) # (seq_len, input_window, feature)\n","        Y_src = Y_src.reshape(Y_src.shape[0], Y_src.shape[1], num_feature).transpose((1,0,2)) # (seq_len, output_window, feature)\n","        Y_tgt = Y_tgt.reshape(Y_tgt.shape[0], Y_tgt.shape[1], num_feature).transpose((1,0,2))\n","        self.x = X[:,:-1,:]\n","        self.y_src = Y_src\n","        self.y_tgt = Y_tgt\n","        self.len = len(X)\n","\n","    def __getitem__(self, i):\n","        return self.x[i], self.y_src[i], self.y_tgt[i]\n","\n","    def __len__(self):\n","        return self.len\n","\n","\"\"\"\n","model\n","\n","attn_forward\n","- multihead attention layer 에서 attention energy 값 저장하도록 class 수정\n","- need_weights=True 로 설정\n","\n","\"\"\"\n","\n","\n","def attn_forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,\n","            need_weights: bool = True, attn_mask: Optional[torch.Tensor] = None,\n","            average_attn_weights: bool = True) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n","    is_batched = query.dim() == 3\n","    if self.batch_first and is_batched:\n","        query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\n","\n","    if not self._qkv_same_embed_dim:\n","        attn_output, attn_output_weights = F.multi_head_attention_forward(\n","            query, key, value, self.embed_dim, self.num_heads,\n","            self.in_proj_weight, self.in_proj_bias,\n","            self.bias_k, self.bias_v, self.add_zero_attn,\n","            self.dropout, self.out_proj.weight, self.out_proj.bias,\n","            training=self.training,\n","            key_padding_mask=key_padding_mask, need_weights=True,\n","            attn_mask=attn_mask, use_separate_proj_weight=True,\n","            q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n","            v_proj_weight=self.v_proj_weight, average_attn_weights=average_attn_weights)\n","    else:\n","        attn_output, attn_output_weights = F.multi_head_attention_forward(\n","            query, key, value, self.embed_dim, self.num_heads,\n","            self.in_proj_weight, self.in_proj_bias,\n","            self.bias_k, self.bias_v, self.add_zero_attn,\n","            self.dropout, self.out_proj.weight, self.out_proj.bias,\n","            training=self.training,\n","            key_padding_mask=key_padding_mask, need_weights=True,\n","            attn_mask=attn_mask, average_attn_weights=average_attn_weights)\n","    \n","    # property 추가\n","    self.attn = attn_output_weights\n","\n","    if self.batch_first and is_batched:\n","        return attn_output.transpose(1, 0), attn_output_weights\n","    else:\n","        return attn_output, attn_output_weights\n","\n","class transformer(nn.Module):\n","    def __init__(self, d_model, nhead, num_encoder_layers=1, num_decoder_layers=1, dim_feedforward=128): \n","        super(transformer, self).__init__() \n","        self.d_model = d_model\n","        self.nhead = nhead\n","        self.num_encoder_layers = num_encoder_layers\n","        self.num_decoder_layers = num_decoder_layers\n","        self.dim_feedforward = dim_feedforward\n","\n","        # batch_first=True 인 경우 (batch, input_window, feature) 에 맞춰서 input\n","        self.transformer = nn.Transformer(d_model=self.d_model, \n","                                          nhead=self.nhead,\n","                                          num_encoder_layers=self.num_encoder_layers,\n","                                          num_decoder_layers=self.num_decoder_layers,\n","                                          dim_feedforward=self.dim_feedforward,\n","                                          batch_first=True) \n","        # self.linear = nn.Linear(self.d_model, 1)\n","\n","    def forward(self, src, tgt):\n","        output = self.transformer(src, tgt)\n","        # output2 = self.linear(output1)\n","        return output\n","\n","\"\"\"\n","train\n","\"\"\"\n","\n","# decoding 을 max len 만큼 반복해야해서 시간이 훨씬 길어짐\n","def train(model, train_loader, max_len, optimizer, criterion, device='cuda'):\n","    model.train()\n","    total_loss = 0.0\n","    attention_list = []\n","\n","    for x, y_in, y_out in train_loader:\n","        optimizer.zero_grad()\n","        x = x.to(device).float()\n","        y_in = y_in.to(device).float()\n","        y_out = y_out.to(device).float()\n","\n","        y_init = x[:,-1,:].unsqueeze(1)\n","        y_in = y_init.clone().detach()\n","        for i in range(max_len):\n","            output = model(x, y_in).to(device)\n","            y_in = torch.cat([y_init, output], dim=1) \n","        attention_list.append(list(model.transformer.decoder.layers[0].multihead_attn.attn.detach().cpu().numpy()))\n","        # attention_list.append(list(model.transformer.decoder.layers[0].multihead_attn.attn))\n","\n","        loss = criterion(output, y_out)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.cpu().item()\n","    train_loss = total_loss/len(train_loader)\n","\n","    return output.detach().cpu().numpy(), y_out.detach().cpu().numpy(), np.array(sum(attention_list,[])), train_loss\n","\n","\"\"\"\n","predict\n","\"\"\"\n","\n","# def predict(model, test_loader, tgt_mask, criterion, device='cuda', file_name=None):\n","#     model.eval()\n","\n","#     total_loss = 0.0\n","#     outputs = []\n","#     ys = []\n","#     for t, (x, y_in, y_out) in enumerate(test_loader):\n","#         # print(t)\n","#         x = x.to(device).float()\n","#         y_in = y_in.to(device).float()\n","#         y_out = y_out.to(device).float()\n","#         output = model(x, y_in, tgt_mask=tgt_mask).to(device)\n","#         outputs.append(list(output.detach().cpu().numpy()))\n","#         ys.append(list(y_out.detach().cpu().numpy()))\n","#         loss = criterion(output[:,:,0], y_out[:,:,0])\n","#         total_loss += loss.cpu().item()\n","#     test_loss = total_loss/len(test_loader)\n","\n","#     return np.array(sum(outputs,[])), np.array(sum(ys,[])), test_loss\n","\n","\n","def predict(model, test_loader, max_len, criterion, device='cuda', file_name=None):\n","    model.eval()\n","\n","    total_loss = 0.0\n","    outputs = []\n","    ys = []\n","    attention_list = []\n","\n","    for t, (x, y_in, y_out) in enumerate(test_loader):\n","        # print(t)\n","        x = x.to(device).float()\n","        y_in = y_in.to(device).float()\n","        y_out = y_out.to(device).float()\n","\n","        y_init = x[:,-1,:].unsqueeze(1)\n","        y_in = y_init.detach()\n","        for i in range(max_len):\n","            output = model(x, y_in).to(device)\n","            y_in = torch.cat([y_init, output], dim=1) \n","        attention_list.append(list(model.transformer.decoder.layers[0].multihead_attn.attn.detach().cpu().numpy()))\n","        outputs.append(list(output.detach().cpu().numpy()))\n","        ys.append(list(y_out.detach().cpu().numpy()))\n","        loss = criterion(output, y_out)\n","        total_loss += loss.cpu().item()\n","    test_loss = total_loss/len(test_loader)\n","\n","    return np.array(sum(outputs,[])), np.array(sum(ys,[])), np.array(sum(attention_list, [])), test_loss"]}]}